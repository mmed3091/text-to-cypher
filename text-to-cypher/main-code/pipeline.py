import requests
from bs4 import BeautifulSoup
import os
import openai
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import cohere
import re
import json
import time
from datetime import datetime
from llamaapi import LlamaAPI
import anthropic


WIKI_TXT = ""
OPENAI_API_KEY = "key"
URLS = []



def get_text_from_file(filename):
    content = ""

    with open(filename, 'r') as file:
        content = file.read()

    return content


def get_wiki_txt(url, max_word_count):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36"
    }

    # Request the page
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Ensure we notice bad responses
    bsoup = BeautifulSoup(response.text, 'html.parser')
    paragraphs = bsoup.find_all('p')

    # Accumulate text until reaching the max word count
    wiki_text = []
    total_words = 0

    for p in paragraphs:
        paragraph_text = p.get_text()
        word_count = len(paragraph_text.split())
        
        if total_words + word_count <= max_word_count:
            wiki_text.append(paragraph_text)
            total_words += word_count
        else:
            # If adding this paragraph exceeds max_word_count, add only part of it
            words_needed = max_word_count - total_words
            partial_text = ' '.join(paragraph_text.split()[:words_needed])
            wiki_text.append(partial_text)
            break

    return ' '.join(wiki_text)




#################################################
############### OPENAI QUERIES ##################
#################################################
"""Returns cypher queries generated by openai"""
def openai_queries(wiki_txt):

    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY  
    openai.api_key = os.environ["OPENAI_API_KEY"]


    # Retrieve example queries
    example_queries = get_text_from_file('example_queries.txt')

    # Prompts
    prompt = f"""- Return Cypher queries for creating knowledge graphs encapsulating relationships between entities identified from the given text.
            - Ensure that all relationships between identified entities are created - e.g. multiple relationship types, etc.
            - Ensure that all entities have correct attributes and relationships reflected in the Cypher queries.
            - Only include Cypher queries in your answer.
            - Use the example queries for reference: {example_queries}.
            Text: {wiki_txt}"""
    

    messages = [
        {"role": "user", "content": prompt}
    ]

    answer = None 

    for attempt in range(5):  # Allow up to 5 attempts
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=messages
            )
            answer = response.choices[0].message['content'].strip().lower()
            break  # Exit loop if successful
        except openai.error.RateLimitError:
            wait_time = 20 * (2 ** attempt)  
            print(f"Rate limit reached. Waiting for {wait_time} seconds...")
            time.sleep(wait_time)
        except Exception as e:
            print(f"Error querying OpenAI: {e}")
            break  # Exit loop on other errors

    return answer






################################################
############### COHERE RESPONE #################
################################################

def cohere_api(queries, text):
    cohere_client = cohere.Client("key") 

    prompt = (
        f"""Analyse the following cypher queries and identify any mistakes or errors "
        such as syntax errors, single entities with no relationships, etc. based on 
        the provided text: Queries: {queries}, Text: {text}. 
        Note: Do not include Cypher Queries in your response."""
    )

    for attempt in range(3):  # Retry up to 3 times
        try:
            response = cohere_client.generate(
                model='command-r-plus',  
                prompt=prompt,
                max_tokens=150  
            )
            return response.generations[0].text.strip()  # Return the response text

        except cohere.ApiError as e:
            if e.status_code == 500:
                wait_time = 10 * (2 ** attempt)  
                print(f"Server error. Waiting for {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                print(f"Error with API request: {e}")
                break  # Exit on other API errors
        except Exception as e:
            print(f"Unexpected error: {e}")
            break  # Exit on unexpected errors

    return None  # Return None if the request fails after retries



################################################
############### GEMINI QUERIES #################
################################################

"""Returns corrected queries"""
def gemini_correct(queries, text, errors):


    if "GOOGLE_API_KEY" not in os.environ:
        os.environ["GOOGLE_API_KEY"] = "key"
    load_dotenv()
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

    llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=GOOGLE_API_KEY)
    cypher_prompt = PromptTemplate.from_template("You are a cypher command corrector. 1. Correct given cypher queries based on mistakes and errors identified, as well as any errors mistakes you identify based on the provided text from which queries were generated. Errors: {errors} 2. Text: {text} 3. Queries to correct: {queries}. DO NOT correct the text, ONLY the cypher queries, and return only corrected cypher queries")
    cypher_chain = LLMChain(llm=llm, prompt=cypher_prompt, verbose=True)
    resp = cypher_chain.run(errors=errors,text=text, queries=queries)

    return resp
    
    

def pipeline_execution(wiki_text):
    answer = openai_queries(wiki_text) # queries
    errors = cohere_api(answer, wiki_text) # error analysis
    queries = gemini_correct(answer, wiki_text, errors) # correct queries using gemini

    return queries




def analyse_queries_and_get_metrics(text, generated_queries, api_key):

    if "OPENAI_API_KEY" not in os.environ:
        os.environ["OPENAI_API_KEY"] = api_key
    load_dotenv()

    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    # Prepare the prompt for OpenAI GPT-4
    prompt_template = f"""Given the following queries:
    {generated_queries}

    And the reference text from which the queries were generated:
    {text}

    Provide results in the following JSON format enclosed in brackets. 
    VERY IMPORTANT: Ensure that only the following fields are included and DO NOT add any additional comments or notes NOR EXPLATIONS. 
    Use the following as a guide for generating the results:

    1. Syntax Errors: Count any errors that may prevent the queries from being run.
    2. Correctly/Incorrectly Created Nodes: Compare the nodes against the reference text to check if all or any nodes correctly or incorrectly reflect entities from the text.
    3. Correctly/Incorrectly Created Relationships: Compare the relationships against the reference text to check if the relationships between two nodes should exist, regardless of the label.
    4. Correctly/Incorrectly Labelled Relationships: Compare against the reference text to see if the relationship labels correctly represent what is mentioned.

    The results you provide must be ready for JSON parsing without any errors or comments, in the following format:

    {{
        "Syntax Errors": number,
        "Correctly Created Nodes": number,
        "Incorrectly Created Nodes": number,
        "Correctly Created Relationships": number,
        "Incorrectly Created Relationships": number,
        "Correctly Labelled Relationships": number,
        "Incorrectly Labelled Relationships": number
    }}
    """



    openai.api_key = OPENAI_API_KEY
    response_content = None


    for attempt in range(5):  # Allow up to 5 attempts
        try:
            # Call OpenAI's GPT-4 to analyse queries and calculate metrics
            response = openai.ChatCompletion.create(
                model="gpt-4",  
                messages=[
                    {"role": "user", "content": prompt_template}
                ]
            )
            response_content = response['choices'][0]['message']['content']
            break  # Exit the loop if successful
        except Exception as e:
            print(f"An error occurred: {e}")
            time.sleep(20 * (2 ** attempt))  

    # parsing the JSON string into a Python dictionary
    try:
        response_dict = json.loads(response_content)
    except json.JSONDecodeError as e:
        print("Failed to parse JSON from the response:", e)
        response_dict = {}

    return response_dict



def best_and_worst(json_file):
    DICT_ENTRIES = ["syntax",
                "correct_nodes", 
                "incorrect_nodes", 
                "correct_relationships",
                "incorrect_relationships",
                "correct_relationship_labels",
                "incorrect_relationship_labels"
                ]

    highest = {}
    lowest = {}
    overall_best = ["", float("-inf")]
    overall_worst = ["", float("-inf")]

    # Create dictionaries 
    for entry in DICT_ENTRIES:
        highest_values = [float("-inf"), ""]
        lowest_values = [float("inf"), ""]
        highest[entry] = highest_values
        lowest[entry] = lowest_values

 
    # Read the JSON file
    with open(json_file, 'r') as file:
        data = json.load(file)  

    # Loop through the entries
    for entry in data:

        summary_values = entry.get('summary')
        url = entry.get('url')
        total_nodes = summary_values.get('Correctly Created Nodes') + summary_values.get('Incorrectly Created Nodes')
        total_relationships = summary_values.get('Correctly Created Relationships') + summary_values.get('Incorrectly Created Relationships')
        total_rel_labels = summary_values.get('Correctly Labelled Relationships') + summary_values.get('Incorrectly Labelled Relationships')

        
        # Calculations
        syntax = summary_values.get('Syntax Errors')
        if(total_nodes > 0):
            correct_nodes = round((summary_values.get('Correctly Created Nodes')/total_nodes) * 100, 2)
            incorrect_nodes = round((summary_values.get('Incorrectly Created Nodes')/total_nodes) * 100, 2)
        else:
            correct_nodes = 0
            incorrect_nodes = 0
        
        if(total_relationships > 0):
            correct_relationships = round((summary_values.get('Correctly Created Relationships')/total_relationships)* 100, 2)
            incorrect_relationships = round((summary_values.get('Incorrectly Created Relationships')/total_relationships)* 100, 2)
        else:
            correct_relationships = 0
            incorrect_relationships = 0
        
        if(total_rel_labels > 0):
            correct_relationship_labels = round((summary_values.get('Correctly Labelled Relationships')/total_rel_labels) * 100, 2)
            incorrect_relationship_labels = round((summary_values.get('Incorrectly Labelled Relationships')/total_rel_labels) * 100, 2)
        else:
            correct_relationship_labels = 0
            incorrect_relationship_labels = 0

        total = 0.00
        correct_average = 0.00
        incorrect_average = 0.00
        for statistic_name in DICT_ENTRIES:
            if(locals()[statistic_name] > highest[statistic_name][0]):
                highest[statistic_name][0] = locals()[statistic_name]
                highest[statistic_name][1] = url

            if(locals()[statistic_name]  < lowest[statistic_name][0]):
                lowest[statistic_name][0] = locals()[statistic_name]
                lowest[statistic_name][1] = url

            val = locals()[statistic_name]
            if(statistic_name != "syntax"):
                val = val/100

            if("correct" in statistic_name and "incorrect" not in statistic_name):
                correct_average += val
            else:
                incorrect_average += val

        correct_average = round(correct_average/3, 2)
        incorrect_average = round(incorrect_average/4, 2)
        

        if(correct_average > overall_best[1]):
            overall_best[0] = url
            overall_best[1] = correct_average

        if(incorrect_average > overall_worst[1]):
            overall_worst[0] = url
            overall_worst[1] = incorrect_average

    file = json_file
    results_file = file.replace("json", "txt")

    # Write results to a text file
    with open(results_file, "w") as f:
        f.write("Highest values: \n")
        for key, value in highest.items():
            result = value[0]  
            url = value[1]  
            f.write(f"{key}: {result}, ({url})\n")

        f.write("\nLowest values: \n")
        for key, value in lowest.items():
            result = value[0]  
            url = value[1]  
            f.write(f"{key}: {result}, ({url})\n")

        f.write(f"\nHighest Accuracy Score: {overall_best[1]}, {overall_best[0]}")
        f.write(f"\nHighest Error Score: {overall_worst[1]}, {overall_worst[0]}")





def process_urls(URLS, text_length, output_filename, api_key):

    # Initialise responses
    all_responses = []

    # Loop through each URL for Cypher queries generation
    for url in URLS:
        text = get_wiki_txt(url, text_length)
        filename = "file.txt"
        with open(filename, "w") as f:
            f.write(text)
        start_time = time.time()
        final_queries = pipeline_execution(text)
        finish_time = time.time()
        execution_time = finish_time - start_time
        summary = analyse_queries_and_get_metrics(text, final_queries, api_key)

        # Add results to response variable
        all_responses.append({
            "url": url,
            "queries": final_queries,
            "summary": summary,
            "execution_time_in_seconds": execution_time
        })

    # Add results to a json file
    with open(output_filename, 'w') as json_file:
        json.dump(all_responses, json_file, indent=4)

    best_and_worst(output_filename)




def process_urls_evaluation(URLS, text_length, output_filename, api_key, function):
    
    # Initialise responses
    all_responses = []

    # Loop through each URL for Cypher queries generation
    for url in URLS:
        text = get_wiki_txt(url, text_length)
        start_time = time.time()
        final_queries = function(text)
        finish_time = time.time()
        execution_time = finish_time - start_time
        summary = analyse_queries_and_get_metrics(text, final_queries, api_key)

        # Add results to response variable
        all_responses.append({
            "url": url,
            "queries": final_queries,
            "summary": summary,
            "execution_time_in_seconds": execution_time
        })

    # Add results to a json file
    with open(output_filename, 'w') as json_file:
        json.dump(all_responses, json_file, indent=4)

    best_and_worst(output_filename)



def gemini_evaluation(text):

    if "GOOGLE_API_KEY" not in os.environ:
        os.environ["GOOGLE_API_KEY"] = "key"
    load_dotenv()
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

    llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=GOOGLE_API_KEY)

    cypher_prompt = PromptTemplate.from_template("Generate Cypher queries for creating knowledge graphs encapsulating relationships between entities identified from the given text: {text}")

    cypher_chain = LLMChain(llm=llm, prompt=cypher_prompt, verbose=True)


    resp = cypher_chain.run(text=text)
    return resp



def cohere_evaluation(text):
    cohere_client = cohere.Client("key")

    prompt = f"Generate Cypher queries for creating knowledge graphs encapsulating relationships between entities identified from the given text: {text}"


    response = cohere_client.generate(
        model='command-r-plus',
        prompt=prompt,
    )
    
    return response.generations[0].text.strip()





def openai_evaluation(text):


    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY 
    openai.api_key = os.environ["OPENAI_API_KEY"]


    prompt = f"Generate Cypher queries for creating knowledge graphs encapsulating relationships between entities identified from the given text: {text}"

    messages = [
        {"role": "user", "content": prompt}
    ]

    answer = None 

    for attempt in range(5):  # Allow up to 5 attempts
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=messages
            )
            answer = response.choices[0].message['content'].strip().lower()
            break  # Exit loop if successful
        except openai.error.RateLimitError:
            wait_time = 20 * (2 ** attempt) 
            print(f"Rate limit reached. Waiting for {wait_time} seconds...")
            time.sleep(wait_time)
        except Exception as e:
            print(f"Error querying OpenAI: {e}")
            break  # Exit loop on other errors

    return answer


def claude_evaluation(text):

    client = anthropic.Anthropic(api_key="key")
    prompt = f"Generate CYPHER QUERIES for creating knowledge graphs encapsulating relationships between entities identified from the given text: {text}"

    # Create the message
    message = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=3000,
        temperature=1,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    }
                ]
            }
        ]
    )

    # Return the response content
    return message.content[0].text






if __name__ == "__main__":

    # Get links from the file
    with open('links.txt', 'r') as file:
        links = [line.strip() for line in file]
    URLS = links


    # Benchmark
    process_urls(URLS, 3000, 'responses_1.json', OPENAI_API_KEY)
    process_urls(URLS, 3500, 'responses_2.json', OPENAI_API_KEY)


    # LLMs
    process_urls_evaluation(URLS, 3000, 'openai_evaluation.json', OPENAI_API_KEY, openai_evaluation)
    process_urls_evaluation(URLS, 3500, 'openai_evaluation_2.json', OPENAI_API_KEY, openai_evaluation)
    process_urls_evaluation(URLS, 3000, 'gemini_evaluation.json', OPENAI_API_KEY, gemini_evaluation)
    process_urls_evaluation(URLS, 3500, 'gemini_evaluation_2.json', OPENAI_API_KEY, gemini_evaluation)
    process_urls_evaluation(URLS, 3000, 'cohere_evaluation.json', OPENAI_API_KEY, cohere_evaluation)
    process_urls_evaluation(URLS, 3500, 'cohere_evaluation_2.json', OPENAI_API_KEY, cohere_evaluation)
    process_urls_evaluation(URLS, 3000, 'claude_evaluation.json', OPENAI_API_KEY, claude_evaluation)
    process_urls_evaluation(URLS, 3500, 'claude_evaluation_2.json', OPENAI_API_KEY, claude_evaluation)











    
        

    